{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baa8432f",
   "metadata": {},
   "source": [
    "# Truth Hunter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eb29ed0",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains Python code that aims to fact-check an article using similar articles. The code uses various Python libraries such as NLTK, TensorFlow, and others for text processing, web scraping, and machine learning tasks. Below are the steps involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69bb0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup, NavigableString, Comment\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "import re  # Added for better text sanitization\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48b14ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cache dictionary to store titles and content for URLs\n",
    "url_cache = {}\n",
    "content_cache = {}\n",
    "\n",
    "# Load Sentence Embedding Model\n",
    "embed = hub.load(\"/Users/salmanshanavas/universal-sentence-encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1001cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a list of sentences and returns their Universal Sentence Encoder embeddings.\n",
    "def get_universal_sentence_embeddings(sentences):\n",
    "    return embed(sentences).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7204b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a URL, this function fetches the full HTML content of the webpage. It returns the HTML as a text string.\n",
    "def get_full_html_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "    finally:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8002013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates HTML code to highlight sentences within an article that may be misleading.\n",
    "def highlight_sentence(sentence, soup_element, similar_article_links=[]):\n",
    "    highlight_style = \"background-color: yellow;\"\n",
    "    popup_content = f\"<strong>Misleading Info</strong><br>Please visit:<br>{'<br>'.join([f'<a href={link}>{link}</a>' for link in similar_article_links])}\"\n",
    "    highlighted_text = f'<span class=\"highlighted\" style=\"{highlight_style}\" data-popup-content=\"{popup_content}\">{sentence}</span>'\n",
    "    new_content = BeautifulSoup(highlighted_text, 'html.parser')\n",
    "    return new_content if new_content else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ded9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the HTML of an article and a list of discrepancies. It returns the HTML with the discrepancies highlighted and a popup containing similar articles.\n",
    "def highlight_discrepancies_in_html(original_html, discrepancies, similar_articles):\n",
    "    # Parse the original HTML content\n",
    "    soup = BeautifulSoup(original_html, 'html.parser')\n",
    "    \n",
    "    # Add CSS styles for the hover-over popup\n",
    "    css_style = \"\"\"\n",
    "    .highlighted:hover .popup {\n",
    "        display: block;\n",
    "    }\n",
    "    .popup {\n",
    "        display: none;\n",
    "        position: absolute;\n",
    "        background-color: #f9f9f9;\n",
    "        border: 1px solid #ccc;\n",
    "        border-radius: 4px;\n",
    "        box-shadow: 0 8px 16px 0 rgba(0,0,0,0.2);\n",
    "        z-index: 1;\n",
    "        padding: 10px;\n",
    "        font-size: small;\n",
    "    }\n",
    "    \"\"\"\n",
    "    style_tag = soup.new_tag(\"style\", type=\"text/css\")\n",
    "    style_tag.string = css_style\n",
    "    soup.head.append(style_tag)\n",
    "\n",
    "    # Add JavaScript for the popup\n",
    "    js_script = \"\"\"\n",
    "    document.addEventListener(\"DOMContentLoaded\", function() {\n",
    "        var highlights = document.querySelectorAll(\".highlighted\");\n",
    "        highlights.forEach(function(highlight) {\n",
    "            var popupContent = highlight.getAttribute(\"data-popup-content\");\n",
    "            var popup = document.createElement(\"div\");\n",
    "            popup.className = \"popup\";\n",
    "            popup.innerHTML = popupContent;\n",
    "            highlight.appendChild(popup);\n",
    "        });\n",
    "    });\n",
    "    \"\"\"\n",
    "    script_tag = soup.new_tag(\"script\", type=\"text/javascript\")\n",
    "    script_tag.string = js_script\n",
    "    soup.body.append(script_tag)\n",
    "\n",
    "    # Highlight discrepancies in the article\n",
    "    for element in soup.find_all(string=True):\n",
    "        if not isinstance(element, NavigableString):\n",
    "            continue\n",
    "        for sentence in discrepancies:\n",
    "            if sentence in element.string:\n",
    "                parent = element.find_parent()\n",
    "                similar_article_links = [article['url'] for article in similar_articles[:3]]\n",
    "                new_element = highlight_sentence(sentence, element.string, similar_article_links)\n",
    "                if new_element and parent:\n",
    "                    new_soup_element = BeautifulSoup(str(new_element), 'html.parser')\n",
    "                    element.replace_with(new_soup_element)\n",
    "                    \n",
    "    # Save the modified HTML content\n",
    "    html_path = 'highlighted_article.html'\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(soup))\n",
    "    \n",
    "    return html_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dc4ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validates the format of a URL.\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "feec7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches and returns the title of the article located at a given URL.\n",
    "def get_title_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    if url in url_cache:\n",
    "        return url_cache[url]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers.get('Content-Type')\n",
    "        if 'text' not in content_type:\n",
    "            return \"Content is not textual\"\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.title\n",
    "        if title:\n",
    "            cleaned_title = title.string.strip()\n",
    "            cleaned_title = cleaned_title.split(\"|\")[0].strip()\n",
    "            url_cache[url] = cleaned_title\n",
    "            return cleaned_title\n",
    "        else:\n",
    "            return \"No title found\"\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "\n",
    "    finally:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ed43fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetches and returns the main content of the article at a given URL.\n",
    "def get_content_from_url(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    if url in content_cache:\n",
    "        return content_cache[url]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers.get('Content-Type')\n",
    "        if 'text' not in content_type:\n",
    "            return \"Content is not textual\"\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for element in soup(['script', 'style', 'noscript', 'header', 'footer', 'aside']):\n",
    "            element.extract()\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        main_content = ''\n",
    "        tags_to_check = ['article', 'div', 'p', 'section']\n",
    "        for tag_name in tags_to_check:\n",
    "            tags = soup.find_all(tag_name)\n",
    "            for tag in tags:\n",
    "                tag_text = tag.text.strip()\n",
    "                if len(tag_text.split()) > len(main_content.split()):\n",
    "                    main_content = tag_text\n",
    "\n",
    "        if not main_content:\n",
    "            return \"Error: Unable to extract article content.\"\n",
    "\n",
    "        content_cache[url] = main_content\n",
    "        return main_content\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error fetching URL: {e}\"\n",
    "\n",
    "    finally:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "338de703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Takes an article title and refines it to form a query string that can be used to find similar articles.\n",
    "def refine_title_v6(title, exclude_words=[]):\n",
    "   # Text sanitization for better refinement\n",
    "    title = re.sub('[^a-zA-Z\\s]', '', title).lower()\n",
    "    \n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(title)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Named Entity Recognition\n",
    "    named_entities = [chunk[0][0] for chunk in ne_chunk(tagged_tokens) if hasattr(chunk, 'label')]\n",
    "    \n",
    "    # Fall back to nouns and verbs if not enough named entities\n",
    "    if len(named_entities) < 2:\n",
    "        nouns_verbs = [word for word, pos in tagged_tokens if pos[:2] in ['NN', 'VB']]\n",
    "        named_entities = nouns_verbs\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Uninformative phrases or words\n",
    "    uninformative_phrases = ['york', 'times', 'bbc', 'cnn', 'fox', 'news', 'daily', 'post', 'guardian']\n",
    "    \n",
    "    # Filtering tokens\n",
    "    filtered_tokens = [word for word in named_entities if word.lower() not in stop_words and word.lower() not in uninformative_phrases and word.lower() not in exclude_words and word.isalnum()]\n",
    "    \n",
    "    # Create query string\n",
    "    query = '+'.join(filtered_tokens)\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ccb26265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tries to find similar articles by refining the query until similar articles are found.\n",
    "def find_similar_articles(original_title):\n",
    "    # List to keep track of words to exclude in subsequent retries\n",
    "    exclude_words = []\n",
    "    \n",
    "    # Retry loop\n",
    "    while True:\n",
    "        # Refine the title\n",
    "        refined_title = refine_title_v6(original_title, exclude_words)\n",
    "        \n",
    "        print(f\"Trying with refined title: {refined_title}\")\n",
    "        \n",
    "        # Identify a word to exclude in the next retry (e.g., the last word in the refined title)\n",
    "        if '+' in refined_title:\n",
    "            word_to_exclude = refined_title.split('+')[-1]\n",
    "            exclude_words.append(word_to_exclude)\n",
    "        else:\n",
    "            print(\"No more words to exclude. Exiting.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fca6479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetches similar articles from the News API.\n",
    "def get_similar_articles_from_api(user_title, api_key):\n",
    "    base_url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"qInTitle\": user_title,\n",
    "        \"apiKey\": api_key,\n",
    "        \"sortBy\": \"relevancy\",\n",
    "        \"pageSize\": 10\n",
    "    }\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['status'] != 'ok':\n",
    "        print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "        return []\n",
    "\n",
    "    if not data['articles']:\n",
    "        params = {\n",
    "            \"q\": user_title,\n",
    "            \"apiKey\": api_key,\n",
    "            \"sortBy\": \"relevancy\",\n",
    "            \"pageSize\": 10\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if data['status'] != 'ok':\n",
    "            print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "            return []\n",
    "\n",
    "    # Filter out articles that have \"[Removed]\" in their title or content\n",
    "    similar_articles = [\n",
    "        {\n",
    "            \"title\": article['title'], \n",
    "            \"url\": article['url'], \n",
    "            \"content\": article.get('content', '')\n",
    "        } \n",
    "        for article in data['articles'] \n",
    "        if article.get('content') and \"[Removed]\" not in article['title'] and \"[Removed]\" not in article.get('content', '')\n",
    "    ]\n",
    "\n",
    "    return similar_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bf5f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits content into sentences for easier processing (Used in classify and highlight functions)\n",
    "def split_content_into_sentences(content):\n",
    "    return sent_tokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b933e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes the user's article and a list of similar articles, then classifies the user's article as \"true\" or \"misleading\". It also identifies discrepancies and potentially biased sentences.\n",
    "def classify_and_highlight_article(user_content, similar_articles, classification_threshold=0.75, discrepancy_threshold=0.50, early_stop_threshold=5):\n",
    "    bias_keywords = ['fake', 'conspiracy', 'hoax', 'untrustworthy']  \n",
    "    \n",
    "    if not similar_articles:\n",
    "        print(\"No similar articles found for comparison.\")\n",
    "        return \"undetermined\", [], []\n",
    "\n",
    "    user_sentences = split_content_into_sentences(user_content)\n",
    "    user_sentence_embeddings = get_universal_sentence_embeddings(user_sentences)\n",
    "    \n",
    "    overall_similarities = []\n",
    "    \n",
    "    for article in similar_articles:\n",
    "        similar_sentences = split_content_into_sentences(article['content'])\n",
    "        \n",
    "        if not similar_sentences:\n",
    "            print(\"Skipping an article with no content.\")\n",
    "            continue\n",
    "        \n",
    "        similar_sentence_embeddings = get_universal_sentence_embeddings(similar_sentences)\n",
    "        \n",
    "        sentence_similarities = np.dot(user_sentence_embeddings, similar_sentence_embeddings.T).max(axis=1)\n",
    "        \n",
    "        overall_similarities.append(sentence_similarities)\n",
    "    \n",
    "    if not overall_similarities:\n",
    "        print(\"No similarities could be computed.\")\n",
    "        return \"undetermined\", [], []\n",
    "\n",
    "    overall_similarities = np.array(overall_similarities)\n",
    "    avg_similarities = np.mean(overall_similarities, axis=0)\n",
    "\n",
    "    if np.isscalar(avg_similarities):\n",
    "        avg_similarities = [avg_similarities]\n",
    "\n",
    "    if np.mean(avg_similarities) >= classification_threshold:\n",
    "        classification = \"true\"\n",
    "    else:\n",
    "        classification = \"misleading\"\n",
    "\n",
    "    discrepancies = [user_sentences[i] for i, sim in enumerate(avg_similarities) if sim < discrepancy_threshold]\n",
    "    \n",
    "    # Early stopping\n",
    "    if len(discrepancies) >= early_stop_threshold:\n",
    "        return classification, discrepancies[:early_stop_threshold], []\n",
    "\n",
    "    biased_sentences = [sentence for sentence in user_sentences if any(keyword in sentence.lower() for keyword in bias_keywords)]\n",
    "    \n",
    "    return classification, discrepancies, biased_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b9320d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Live worm found in Australian woman’s brain in world-first discovery\n",
      "Refined Title: worm+found+womans+brain+discovery\n",
      "\n",
      "Similar Articles:\n",
      "Title: Live parasitic worm found in Australian woman’s brain in world first\n",
      "URL: https://www.aljazeera.com/news/2023/8/29/live-parasitic-worm-found-in-australian-womans-brain-in-world-first\n",
      "Content: A live parasitic worm has been found inside the brain of a 64-year-old Australian woman, marking the first case of the infection in humans.\n",
      "The discovery was made by doctors and researchers at the A… [+3425 chars]\n",
      "--------------------------------------------------\n",
      "Title: Live Worm Found In Woman’s Brain Is Rare—But Common Deadly Parasites Already Plague Billions Of People\n",
      "URL: https://www.forbes.com/sites/roberthart/2023/09/01/live-worm-found-in-womans-brain-is-rare-but-common-deadly-parasites-already-plague-billions-of-people/\n",
      "Content: Neurosurgeons in Australia this week reported pulling a live, three-inch worm from a womans brain, a shocking, world-first discovery that adds another threat to the catalog of dangerous worms and par… [+6723 chars]\n",
      "--------------------------------------------------\n",
      "Title: Doctors Remove Live Worm from Australian Woman’s Brain\n",
      "URL: https://learningenglish.voanews.com/a/doctors-remove-live-worm-from-australian-woman-s-brain/7246097.html\n",
      "Content: A brain doctor in Australia received a huge surprise during an operation on a patients brain last year:\n",
      "Gosh, what is that, its moving. Take it out of my hands.\n",
      "That is what Australian neurosurgeon… [+2985 chars]\n",
      "--------------------------------------------------\n",
      "Title: World-first parasitic infection: 3-inch roundworm found in woman’s brain\n",
      "URL: https://newatlas.com/medical/3-inch-roundworm-from-carpet-python-found-in-womans-brain/\n",
      "Content: Researchers have discovered the worlds first case of a new human parasitic infection, pulling a live roundworm picked up from a carpet python from the brain of an Australian woman. They say the case … [+4070 chars]\n",
      "--------------------------------------------------\n",
      "Title: InnovationRx: New Insight Into How The Brain Works\n",
      "URL: https://www.forbes.com/sites/alexknapp/2023/09/06/innovationrx-new-insight-into-how-the-brain-works/\n",
      "Content: InnovationRx is your weekly digest of healthcare news. To get it in your inbox, subscribe here. \n",
      "Neurons appear red and astrocytes appear green in a photo from the Institute for Stem Cell Research. … [+5569 chars]\n",
      "--------------------------------------------------\n",
      "Title: 澳洲婦人腦內的蟒蛇寄生蟲\n",
      "URL: https://pansci.asia/archives/369234\n",
      "Content: [1]Centers for Disease Control and Prevention2023 9 Emerging Infectious Diseases[1, 2][1]\n",
      " 64 20 30 doxycycline 2021 1 3 [2]\n",
      "<ul><li>hemoglobin[3] 115 165 g/L[2][3] 99 g/L[2]</li><li>platelets 150 … [+2704 chars]\n",
      "--------------------------------------------------\n",
      "Title: She was depressed and forgetful. It was the worm in her brain.\n",
      "URL: https://www.bostonglobe.com/2023/08/29/world/she-was-depressed-forgetful-it-was-worm-her-brain/\n",
      "Content: The hunt for the answer, and the alarming discovery, was described this month in Emerging Infectious Diseases, a monthly journal published by the Centers for Disease Control and Prevention.\n",
      "The woma… [+4433 chars]\n",
      "--------------------------------------------------\n",
      "Title: Living Worm Discovered in Australian Patient’s Brain\n",
      "URL: https://www.voanews.com/a/living-worm-discovered-in-australian-patient-s-brain-/7245430.html\n",
      "Content: SYDNEY  An 8-centimeter worm has been found alive in the brain of a woman in Australia, and researchers say it is the first time the parasite has ever been discovered in humans.\n",
      "The worm was extract… [+2463 chars]\n",
      "--------------------------------------------------\n",
      "Title: Live Worm Found ‘Wriggling’ In Woman’s Brain Had Been There For More Than A Year\n",
      "URL: https://brobible.com/culture/article/live-worm-found-womans-brain/\n",
      "Content: iStockphoto\n",
      "In never before seen medical news, a parasitic worm survived in womans body and brain for at least a year and was alive and wriggling when a neurosurgeon discovered it.\n",
      "The woman, 64, l… [+2587 chars]\n",
      "--------------------------------------------------\n",
      "Title: 'Still alive and wriggling:' Doctors remove 3-inch parasitic worm from woman's brain in world first\n",
      "URL: https://www.cnn.com/2023/08/29/australia/australia-parasitic-worm-brain-scn-intl-hnk/index.html\n",
      "Content: When a 64-year-old Australian woman was sent to hospital for brain surgery, neurosurgeon Dr. Hari Priya Bandi was not expecting to pull out a live 8-centimeter (3-inch) long parasitic roundworm that … [+4700 chars]\n",
      "--------------------------------------------------\n",
      "\n",
      "The user's article is classified as: misleading\n",
      "The highlighted article is saved at: highlighted_article.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    url = input(\"Enter the URL of the article: \")\n",
    "    if not is_valid_url(url):\n",
    "        print(\"Invalid URL format.\")\n",
    "        return\n",
    "\n",
    "    title = get_title_from_url(url)\n",
    "    print(f\"Extracted Title: {title}\")\n",
    "    refined_title = refine_title_v6(title)\n",
    "    print(f\"Refined Title: {refined_title}\")\n",
    "\n",
    "    similar_articles = get_similar_articles_from_api(refined_title, \"332ff33479434583874bc9021b68a14b\")\n",
    "\n",
    "    if not similar_articles:\n",
    "        print(\"No similar articles found.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nSimilar Articles:\")\n",
    "    for article in similar_articles:\n",
    "        print(f\"Title: {article['title']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "        print(f\"Content: {article['content']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Get full HTML instead of just content\n",
    "    full_html = get_full_html_from_url(url)\n",
    "    \n",
    "    user_content = get_content_from_url(url)\n",
    "    classification, discrepancies, biased_sentences = classify_and_highlight_article(user_content, similar_articles)\n",
    "    print(f\"\\nThe user's article is classified as: {classification}\")\n",
    "\n",
    "    if biased_sentences:\n",
    "        print(f\"\\nSentences containing potential bias: {biased_sentences}\")\n",
    "\n",
    "    # Highlight discrepancies if any\n",
    "    if discrepancies:\n",
    "        html_path = highlight_discrepancies_in_html(full_html, discrepancies, similar_articles)\n",
    "        print(f\"The highlighted article is saved at: {html_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
